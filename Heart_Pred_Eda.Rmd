---
title: "Heart Attack Predict"
author: "Tim Tantivilaisin"
date: "2023-06-14"
output: pdf_document
---
```{r setup, message = FALSE}
library(dplyr)
library(ggplot2)

library(gridExtra) # for marrangeGrob
```


```{r}
# loading in data
setwd('/Users/timtan/Desktop/Git Portfolio/Heart_Pred/')
df_heart <- read.csv("data/heart.csv")
```

```{r}
print(colnames(df_heart))
```


```{r}
# age
ggplot(data = df_heart, aes(x = age)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black") +
  labs(title = "Age Histogram", x = "Age", y = "Frequency")
```
```{r}
# sex
ggplot(data=df_heart, aes(x=factor(sex))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_x_discrete(labels = c("female", "male")) +
  labs(title = "Sex Histogram", x = "Sex", y = "Frequency")

# chest pain
ggplot(data=df_heart, aes(x=factor(cp))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_x_discrete(labels = c("typical angina", "atypical angina", "non-anginal pain", "asymptomatic")) +
  labs(title = "Chest Pain Histogram", x = "Chest Pain Type", y = "Frequency")

# exng
ggplot(data=df_heart, aes(x=factor(exng))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_x_discrete(labels = c("No", "Yes")) +
  labs(title = "Exercise Induced Angina Histogram", x = "EXIA Type", y = "Frequency")

# caa, or number of blood vessels
ggplot(data=df_heart, aes(x=factor(caa))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  #scale_x_discrete(labels = c("0 Vessels Blocked", "1 Vessels Blocked", "2 Vessels Blocked", "3 Vessels Blocked")) +
  labs(title = "Number of Major Vessels Blocked Histogram", x = "Number Blood Vessels Blocked", y = "Frequency")

# fbs, fasting blood sugar > 120 mg/dl
ggplot(data=df_heart, aes(x=factor(fbs))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_x_discrete(labels = c("< 120 mg/dl", "> 120 mg/dl")) +
  labs(title = "Fasting Blood Sugar > 120 mg/dl Histogram", x = "Fasting Blood Sugar", y = "Frequency")

# restecg, resting electrocardiographic results
ggplot(data=df_heart, aes(x=factor(restecg))) +
  geom_bar(fill = "steelblue", color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_x_discrete(labels = c("Normal", "Having ST-T wave abnormality", " showing probable or definite left ventricular hypertrophy")) +
  labs(title = "Resting Electrocardiographic Results", x = "Results", y = "Frequency")
```

```{r}
hist_columns <- c("age", "trtbps", "chol", "thalachh", "oldpeak", "slp", "thall")

histograms <- list()

# Create histograms for each column using ggplot2
for (col in hist_columns) {
  hist_plot <- ggplot(data = df_heart, aes(x = .data[[col]])) +
    geom_histogram(binwidth = 1, fill = "steelblue", color = "black") +
    labs(title = paste("Histogram of", col), x = col, y = "Frequency")
  
  histograms[[col]] <- hist_plot
}

# Combine and print the histograms as a series
marrangeGrob(histograms, nrow = 3, ncol = 3)
```

So what this really tells us that is we need to convert things to factors.
```{r}
# turning categorical variables to categories
to_factor_cols <- c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall", "output")

df_heart[,to_factor_cols] <- lapply(df_heart[,to_factor_cols], factor)
```


```{r}
library(caret)
# test train split
set.seed(230)
training.individuals <- df_heart$output %>% 
            createDataPartition(p = 0.8, list = FALSE)

train.data <- df_heart[training.individuals, ]
test.data <- df_heart[-training.individuals, ]

# training the model
logistic_model <- glm(output ~ ., 
                   data = train.data, family = 
                     binomial(link="logit"))

# making predictions
logistic_predictions <- logistic_model %>% predict(test.data, type = 'response')
logistic_predictions <- ifelse(logistic_predictions > 0.5, 1, 0)

# Model accuracy
accuracy_logistic <- mean(logistic_predictions==test.data$output)
accuracy_logistic

```

```{r, message=F}
set.seed(230)
library(randomForest)

# training random forest model
rf_model <- randomForest(output ~ ., 
                   data = train.data) 

# making predictions
rf_predictions <- predict(rf_model, test.data)

# getting accuracy
confusion_matrix <- table(Predicted = rf_predictions, Actual = t(test.data$output))
accuracy_rf <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Random Forest Accuracy: ", accuracy_rf)
```


```{r, message=F}
set.seed(230)
library(xgboost)

# xgboost requires variables to be numerics
to_numerics <- c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall", "output")
df_heart[,to_numerics] <- lapply(df_heart[,to_numerics], as.numeric)
df_heart$output <- df_heart$output - 1
# test train split
set.seed(230)
train.data <- df_heart[training.individuals, ]
test.data <- df_heart[-training.individuals, ]

training.individuals <- df_heart$output %>% 
            createDataPartition(p = 0.8, list = FALSE)

# Convert the data to a suitable format for xgboost
train_matrix <- xgb.DMatrix(data = as.matrix(train.data[, -which(names(train.data) == "output")]),
                            label = train.data$output)
test_matrix <- xgb.DMatrix(data = as.matrix(test.data[, -which(names(test.data) == "output")]),
                           label = test.data$output)

# set params for xgboost model
params <- list(
  objective = "binary:logistic", 
  eval_metric = "error",
  max_depth = 6,
  eta = 0.3,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1
)

# training the model
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 100, # number of boosting rounds
  watchlist = list(train = train_matrix, test = test_matrix),
  early_stopping_rounds = 10, # stop if no improvement in test set performance after 10 rounds
  print_every_n = 10 # print evaluation metric every 10 rounds
)

# making predictions
xg_predictions <- predict(xgb_model, test_matrix)

# getting accuracy
xg_predictions <- ifelse(xg_predictions > 0.5, 1, 0)
accuracy_xg <- mean(predicted_labels == test.data$output)
print(accuracy_xg)
```

# naive bayes
```{r}
# Naive Bayes
library(e1071)
library(lattice)
set.seed(254)

train.data$output <- as.factor(train.data$output)
test.data$output <- as.factor(test.data$output)

train_control <- trainControl(method="repeatedcv", number=10, repeats=3)

nb_model <- train(output~., data=train.data, trControl=train_control, 
                  method="naive_bayes")

nb_predictions <- nb_model %>% predict(test.data[, -which(names(test.data) == "output")])
accuracy_nb <- mean(nb_predictions == test.data$output)
print(accuracy_nb)
```

# svm
```{r}
# Set the control parameters for cross-validation
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3)

# Train the SVM model with RBF kernel using cross-validation
svm_model <- train(output ~ ., 
                   data = train.data, 
                   method = "svmRadial", 
                   trControl = ctrl)

# Make predictions using the trained model
svm_predictions <- svm_model %>% predict(newdata = test.data)

# predictions and accuracy
accuracy_svm <- mean(svm_predictions == test.data$output)
print(accuracy_svm)
```


# starting ensemble
```{r}
# data cleaning
nb_predictions <- as.numeric(nb_predictions) - 1
rf_predictions <- as.numeric(rf_predictions) - 1
svm_predictions <- as.numeric(svm_predictions) - 1
ensemble_df <- cbind(logistic_predictions, nb_predictions, rf_predictions, svm_predictions, xg_predictions)

```

```{r}
# for mode function
library(modeest)

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

# getting rowise mode
ensemble_vote <- apply(ensemble_df, 1, getmode)
ensemble_df <- data.frame(cbind(ensemble_df, ensemble_vote))

# predictions and accuracy
accuracy_ensemble <- mean(ensemble_df$ensemble_vote == test.data$output)
print(accuracy_ensemble)
```


```{r}
ensemble_with_answers <- data.frame(ensemble_df, test.data$output)
```


So overall achieved 87% accuracy with this seed.

